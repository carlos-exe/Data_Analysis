# -*- coding: utf-8 -*-
"""Airfoil.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IitTmEBx2gAPX_qclGtNGJbJFcUOvarQ

##Adjunta el archivo airfoil_self_noise
"""

FILEID = "1NgEDDpIWc38KgankSNVxowq3bE1mduPi"
!wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$FILEID -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id="$FILEID -O airfoil_self_noise.dat && rm -rf /tmp/cookies.txt

"""###Arreglo de atributos y definición de la salida"""

# Commented out IPython magic to ensure Python compatibility.
#Paso 1: Lectura
import pandas as pd #https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html
import warnings
warnings.filterwarnings("ignore")

# %matplotlib inline
dat_path = 'airfoil_self_noise.dat'

Xdata = pd.read_csv(dat_path,header = None, delim_whitespace=True, error_bad_lines=False )
#Corroborar si el orden de los nombres si es el correcto.
Xdata.rename(columns = {0:'Frequency', 1:'Angle_of_attack', 2:'Chord_length', 3:'Free-stream_velocity', 4:'Suction_side_displacement_thickness', 5:'Scaled_sound_pressure_level'}, inplace = True)
Xdata.head()

Xdata.info()

#Paso 2: Particion entrenamiento y validacion
# Tamaño Xtrain 70%, Tamaño Xtest 30%
import warnings
warnings.filterwarnings("ignore")

from sklearn.model_selection import train_test_split
Xtrain, Xtest = train_test_split(Xdata,test_size=0.3,random_state=16)
col_sal = "Scaled_sound_pressure_level"
ytrain = Xtrain[col_sal]
ytest = Xtest[col_sal]
Xtrain.drop(columns=col_sal,inplace=True)
Xtest.drop(columns=col_sal,inplace=True)

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="most_frequent")
ytrain = imputer.fit_transform(pd.DataFrame(ytrain))
ytrain = ytrain.reshape(-1)

ytest = imputer.transform(pd.DataFrame(ytest))
ytest = ytest.reshape(-1)

print(ytrain.shape, ytest.shape)

"""En la siguiente celda se obtiene un histograma y un diagrama de caja que muestran los valores más frecuentes de la salida "ytrain"
"""

import matplotlib.pyplot as plt
plt.hist(ytrain,bins = 100)
plt.show()

plt.boxplot(ytrain)
plt.show()

"""Observando así que en el histograma parece haber una gráfica de forma gaussiana, sin embrago, no podemos estar totalmente seguros, entonces observando el diagrama de caja, notamos que efectivamente la mayor cantidad de datos se concentra en 125 pero aun así se encuentra datos atípicos.


A continuación se realiza un análisis descriptivo de los datos.


"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
sca = StandardScaler()
Xtrain_pre_z = sca.fit_transform(Xtrain)

Xtrain_pre_z.var(axis=0)

Xtrain.var(axis=0)

red = PCA()
zz = red.fit_transform(Xtrain_pre_z)

"""#Análisis Descriptivo

En las siguientes celdas se hará un análisis por medio de gráficas como la dada por PCA, la matriz de correlaciones y algunos histogramas, para poder realizar algunas observaciones que pueden conllevar a generar un código mucho más eficaz para el desarrollo del análisis de datos.
"""

plt.scatter(zz[:,0],zz[:,1],c=ytrain,s = 100*Xtrain['Frequency']/(Xtrain['Frequency'].max()))
plt.colorbar()
plt.show()

Xtest['Frequency']

zztest = red.transform(sca.transform(Xtest))
plt.scatter(zz[:,0],zz[:,1],c=ytrain,s = 100*Xtrain['Frequency']/(Xtrain['Frequency'].max()),label='train')
plt.colorbar()
plt.scatter(zztest[:,0],zztest[:,1],c=ytest,s=100*Xtest['Frequency']/Xtest['Frequency'].max(),marker='d',label='test')
plt.legend()
plt.show()

"""###Análisis de correlación"""

corr_matrix = Xtrain.corr()    ## se aplica la función corr que muestra todas las correlaciones entre los atributos
corr_matrix.style.background_gradient(cmap='coolwarm')

import seaborn as sns
sns.heatmap(corr_matrix,xticklabels=corr_matrix.columns.values,yticklabels=corr_matrix.columns.values)
plt.tight_layout()
# plt.savefig('results/corr_matrix.pdf', format='pdf', dpi=300)
plt.show()

"""Con esto se observa que solamente tienen alta correlación Suction_side_displacement_thickness con Angle_of_attack esto posría significar que estos datos nos están dando una información similar, por lo que para optimizar se podría generar una nueva columna de esos datos y eliminar los ya tenidos. Sin embargo, la correlación es muy pequeña, así que los dejamos igual.

A continuación tenemos la gráfica de correlacciones de todos los datos de la base.
"""

from pandas.plotting import scatter_matrix

attributes = ["Frequency","Angle_of_attack","Chord_length","Free-stream_velocity","Suction_side_displacement_thickness"
              ]
scatter_matrix(Xtrain[attributes], figsize=(12, 8))
plt.tight_layout()
# plt.savefig('results/scatter_matrix_plot.pdf', format='pdf', dpi=300)
plt.show()

"""##Preproceso de las entradas de la base de datos."""

# imputer = SimpleImputer(strategy="median")
# Xtrain_pre = imputer.fit_transform(pd.DataFrame(Xtrain))
# Xtrain_pre = Xtrain_pre

# Xtest_pre = imputer.fit_transform(pd.DataFrame(Xtest))
# Xtest_pre = Xtest_pre

# #%% dummy transformer
# from sklearn.base import BaseEstimator, TransformerMixin
# from sklearn.impute import SimpleImputer
# class dummy_air(BaseEstimator,TransformerMixin):
#     def fit(self,X, *_):
#         Xi = X.copy() #copiar dataset para no reemplazar original
#         self.imputer_num = SimpleImputer(strategy="most_frequent") #crear imputador tipo moda
#         self.a = Xi.columns[np.sum(Xi.isna())> 0] #encontrar columnas con datos faltantes
#         self.imputer_num.fit(Xi[self.a]) # ajustar imputador
#         Xi[self.a] = self.imputer_num.transform(Xi[self.a]) #evaluar  sobre datos imputador
#         return self
#     def transform(self, X, *_):#funcion transformador
#         Xi = X.copy()
#         Xi[self.a] = self.imputer_num.transform(Xi[self.a])
#         return Xi
#     def fit_transform(self,X,*_):
#         self.fit(X)
#         return self.transform(X)

"""#Escoger mejor modelo

En las siguientes celdas se escogerá el mejor modelo entre (Regresión lineal, Lasso, Rígido, ElasticNet y KernelRidge) utilizando sintonización de parámetros por búsqueda por grilla, de manera que nos arroje un determinado error para cada modelo, luego hallamos el promedio de errores y se obtiene el mejor modelo basándose en el menor valor de error final. Y el hyperparámetro se lo obtiene con la moda del mejor usado en cada iteración.
"""

from sklearn.linear_model import LinearRegression, ElasticNet, Ridge, Lasso
from sklearn.kernel_ridge import KernelRidge

steps = [
         [('scaler', StandardScaler()), #estandarizar cada atributo columna de xtrain centrada en 0 y var = 1
          ('reg', LinearRegression())],

         [('scaler',StandardScaler()),
          ('reg',Ridge())],

         [('scaler',StandardScaler()),
          ('reg',Lasso())],

         [('scaler',StandardScaler()),
          ('reg',ElasticNet())],

         [('scaler', StandardScaler()), #estandarizar cada atriuto columna de xtrain centrada en 0 y var = 1
          ('reg',  KernelRidge(kernel = 'rbf'))] #clasificador
         ]

#parametros a buscar por busqueda por grilla
parameters = [
             {'reg__fit_intercept':[True, False]   # para el linearregression
             },
             {
              'reg__alpha': [0,1e-3,1e-2,1e-1,1,10] #Para el ridge
             },
             {
              'reg__alpha': [0,1e-3,1e-2,1e-1,1,10] #Para el Lasso
             },
             {
              'reg__alpha': [0,1e-3,1e-2,1e-1,1,10], #parametros n_neighbors debe ser siempre un int
              'reg__l1_ratio':[0,0.25,0.5,0.75,1]
             },
             {
              'reg__alpha': [0,1e-3,1e-2,1e-1,1,10] #Para el Kernel
             }
           ]

label_model = ['Nor+RegLin','Nor+Ridge','Nor+Lasso','Nor+ElasticNet','Nor+KernelRidge']

import os
pathpre = 'datospre'

try:
  os.mkdir(pathpre)
except:
  print("Carpeta results ya existe")

#valildacion cruzada anidada
from joblib import dump, load
from sklearn.metrics import mean_absolute_error as msa
from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score, cross_val_predict
from sklearn.pipeline import Pipeline
import numpy as np
Niter = 6 #numero particiones outter loop nested cross-validation
msev =np.zeros((Niter,len(steps)))#arreglo para guardar acierto/error
Nmod = len(steps) #numero de modelos a probar
best_estimators = Niter*[None]#mejor metodo por iteracion
###clave del funcionamiento
best_hyperpar = Niter*[None]#mejor metodo por iteracion


#############################
for j in range(Niter): #outter loop # SI TIENE MENOS DE 1000 DATOS BORRAR ESTE CICLO SOLO CV EN LINEA 16
      #print('it %d/%d'%(j+1,Niter))
      #particiono datos outter loop
      X_trainj, X_testj, y_trainj, y_testj = train_test_split(Xtrain,ytrain,test_size=0.3) # xtrain 60, xtest 26
      list_est = [] #lista lazo interno para guardar mejor estimador por modelo para iteracion j
      list_hyper = [] #lista lazo interno para guardar mejores hyperparametros por modelo para iteracion j
      for r in range(Nmod): #recorro todos los posibles modelos a probar en iteracion j del outter loop
          grid_search = GridSearchCV(Pipeline(steps[r],memory=pathpre), parameters[r],cv=5,verbose=5,scoring='neg_mean_absolute_error',n_jobs=-1) #cv inner loop
          #xtrain gridsearchcv xtrain split en 12 / cv, 60/5 = 12, xtrain 48 datos validar 12
          # cv = N -> leave one out N <30
          #generar mejor modelo
          grid_search.fit(X_trainj,y_trainj)
          #estimar salida conjunto de test
          y_pred = grid_search.best_estimator_.predict(X_testj)
          #guardar mejor modelo
          list_est.append(grid_search.best_estimator_)
          list_hyper.append(grid_search.best_params_)
          #guardar acierto
          msev[j,r] = msa(y_testj,y_pred)
          print('it %d/%d-Modelo %d/%d'%(j+1,Niter,r+1,len(steps)))
          print('best hyper', grid_search.best_params_)
          print('msa:',msev[j,r])

      best_estimators[j] = list_est #guardar mejores modelos
      best_hyperpar[j] = list_hyper #mejores hyperparametros


      savedata = {
          'acc':msev,
          'best_models':best_estimators,
          'best_parameters':best_hyperpar,
            }
      dump(savedata,'fifa.joblib')

"""##Sintonización de Parámetros

Con los siguientes diagramas de caja se puede observar cuáles valores de parámetros para cada modelo son los mejores y se obtiene una moda para mostrar el mejor valor de cada modelo denotado por "Bestomodelo [  $\alpha$  ]" donde "$\alpha$" es el valor del hyperparámetro.
"""

from scipy.stats import mode
from datetime import date
def bestomodel(r):
  nh = len(best_hyperpar[0][r])
  hyperpar_r = np.zeros((Niter,nh))
  for i in range(Niter):
    for j in range(nh):
      hyperpar_r[i,j] = best_hyperpar[i][r].get(list(best_hyperpar[i][r].keys())[j])



  #revisar numero entero para realizar casting
  aa = list(best_hyperpar[0][r].keys())
  c = -1
  for i in range(len(aa)):
      if aa[i].find('n_neighbors') > -1:
        c = i

  mode_hyper = mode(hyperpar_r,axis=0)[0][0]
  plt.boxplot(hyperpar_r)
  plt.xticks(ticks=np.arange(nh)+1,labels=list(best_hyperpar[0][r].keys()))
  plt.title('Best_hyperparameters '+label_model[r])
  plt.grid()
  plt.show()

  return print("Bestomodelo",mode_hyper)

bestomodel(0),bestomodel(1),bestomodel(2),bestomodel(3),bestomodel(4)

"""A continuación se muestran los errores obtenidos en cada iteración para cada modelo."""

msev

plt.figure(figsize=(10, 5), dpi=90)
plt.boxplot(msev)
plt.xticks(ticks=np.arange(len(steps))+1,labels=label_model)
plt.show()

"""De lo anterior se recopila el error de cada modelo y se halla su promedio, para compararlos y elegir el que provea un menor error, de manera que al final se trabaje con el modelo que arroje el menor valor de error."""

error_metodo = np.zeros(5)
for m in range(5):
  error1 = np.zeros(Niter)
  for n in range(Niter):
    error1[n] = msev[n][m]
    # print(error1)
  error_metodo[m] = np.mean(error1)
  # print(error_metodo)
error_metodo

AA=0
min = error_metodo[0]
for c in range(len(error_metodo)-1):
  if error_metodo[c]>error_metodo[c+1]:
    min = error_metodo[c+1]
    ult = c+1

print("El mejor modelo dado por el criterio de error mínimo es:", label_model[ult], "\nCon un error de:", min)

"""# Modelo definitivo

Con los mejores hiperparámetros entreno sobre Xtrain con el modelo final que se obtiene anteriormente.
"""

r = 4 #camino Kernelridge
steps_final = [('scaler',StandardScaler()),
               ('reg',  KernelRidge(kernel = 'rbf', alpha = 0.001))]


modelo_final = Pipeline(steps_final)
modelo_final.fit(Xtrain,ytrain)

"""# Caso real evaluación datos nuevos

Por último, probamos nuestro modelo final para predecir los valores a la salida con unos nuevos datos.
"""

ytest_e = modelo_final.predict(Xtest) #simular casos nuevos

print('MAE_test=', msa(ytest,ytest_e))

"""A continuación se muestra la gráfica donde se observa la presición que tendrá nuestro modelo para una entrada nueva. Donde se observa que el error fue mínimo, por lo que nuestro modelo se ajusta muy bien a los nuevos datos, así que la predicción es correcta, no del 100% pero sí es muy preciso."""

plt.figure(figsize=(10, 5), dpi=90)
plt.plot(ytest_e,label='prediction')
plt.plot(ytest,label='target', alpha=0.5)
plt.grid()
plt.legend()
plt.show()

"""##Fuentes

1. https://github.com/amalvarezme/AnaliticaDatos
2. https://scikit-learn.org/stable/modules/linear_model.html
"""