# -*- coding: utf-8 -*-
"""AD_proyecto_APmines_MOU_Final

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11NhhCzPYHB6Ab9pn2NasXUmesxjPBwsW

## **Database reading**
- Load database of [Explosive Remnants of War in Colombia, 1982-2013](https://data.world/ocha-colombia/b6d4823e-af32-4549-a9d0-598057d0405a) from drive free access.
"""

FILEID = "1kMnn6FL_oVJY1FO7mmiBETsO9oQ0VgAE"
!wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$FILEID -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id="$FILEID -O codigos.zip && rm -rf /tmp/cookies.txt
!unzip codigos.zip
!dir

"""- Reading - obtaining data."""

import io
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

Xdata = pd.read_excel('minas2.xls')
col_drop = ["Corregimiento","Municipio","Pertenencia a un grupo etnico", "COD_DANE_MUNI", "COD_DANE_DEPTO", "Reporte medico"]
Xdata.drop(columns = col_drop, inplace = True)
Xdata.head()
Xdata

"""- Next it is observed that there are 5 attributes that are of type 'object' (nominal) so it is necessary to make a preprocess on these to determine equivalence of type float."""

Xdata.info()

"""## **Exploratory analysis and visualization on sampled data:**

##### Exploration
"""

dept = Xdata["Departamento"].value_counts()
years = Xdata["Año"].value_counts()

print(years)
print(dept)

canrols =Xdata["Genero"].unique()
canrols[0]

Xdata["Longitud_cabecera"].min(), Xdata["Longitud_cabecera"].max()

"""##### Preprocess categorical attributes"""

cat = ['Tipo de evento', 'Estado','Rango de edad','Condicion', 'Genero', 'Tipo de area', 'Departamento'];
items = []
for i in cat:
    items += [list(Xdata[i].value_counts().index)]
cat_usr = dict(zip(cat, items))

#%% dummy transformer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder,OneHotEncoder

class dummy_mines(BaseEstimator,TransformerMixin):
    #class initialization and varaibles
    def __init__(self, cat_usr):
      self.cat_usr = cat_usr #list of categorical attributes

    def fit(self,X, *_):
        Xi = X.copy() #copy dataset not to replace original
        cat = [] #coding categorical variables with ordinal encoder
        for i in self.cat_usr.keys():
            cat = cat + [[*self.cat_usr.get(i)]]
        self.col_cat_usr = OrdinalEncoder(categories=cat)
        Xi[[*self.cat_usr.keys()]] =self.col_cat_usr.fit_transform(Xi[[*self.cat_usr.keys()]])

        return self

    def transform(self, X, *_):#transformer function
        Xi = X.copy()
        Xi[[*self.cat_usr.keys()]] =self.col_cat_usr.transform(Xi[[*self.cat_usr.keys()]])
        #Xi[self.col_cat]= self.cod.transform(Xi[self.col_cat])
        return Xi

    def fit_transform(self,X,*_):

        self.fit(X)
        return self.transform(X)

"""- Preprocessing class is applied to Xdata."""

dummy = dummy_mines(cat_usr = cat_usr)
X = dummy.fit_transform(Xdata)

"""- It is observed that after preprocessing the attributes are of type float or int and the format has been corrected."""

X.info()

"""### **Interesting graphs**

##### Number of cases of MAP and MUSE per department
"""

plt.figure(figsize = (10,5))
colores = ["#FFD97D"] #"#60D394" green,"#FF9B85" pink "#EE6055" red, "#FFD97D" yellow
dept.plot(kind = 'bar',color = colores)
plt.xlabel('Departament')
plt.ylabel('Number of cases of MAP and MUSE')
plt.title('Number of cases of MAP and MUSE per department')
plt.show()

"""##### Number of MAP and MUSE cases per year and Histogram"""

plt.figure(figsize = (20,5))
plt.subplot(121)
colores = ["#FF9B85"] #"#60D394" green,"#FF9B85" pink "#EE6055" red, "#FFD97D" yellow
years.plot(kind = 'bar',color = colores)
plt.xlabel('Years')
plt.ylabel('Number of cases of MAP and MUSE')
plt.title('Number of cases of MAP and MUSE by year')

plt.subplot(122)
Xdata["Año"].hist()
plt.title('Histogram cases of MAP and MUSE by year')
plt.show()

"""##### Status of people affected by MAP in Colombia

- By means of the correlation between latitude and longitude, the state (Dead, Injured) of the people affected by MAP or MUSE is located in Colombia.
"""

import matplotlib.image as mpimg
colombia_img=mpimg.imread('colombia.png')   ## https://es.wikipedia.org/wiki/Plantilla:Mapa_de_localizaci%C3%B3n_de_Colombia_continental
import matplotlib.pyplot as plt
import matplotlib.colors

prices = (Xdata["Estado"]=="Muerto")

plt.figure(figsize = (15,10.5),dpi = 200)
cmap = matplotlib.colors.ListedColormap(['red', 'blue'])
ax = Xdata.plot(kind="scatter", x="Longitud_cabecera", y="Latitud_cabecera", figsize=(15,10.5),
                  c=prices,cmap=cmap,colorbar=False)

plt.imshow(colombia_img, extent=[-79.349,-66.591,-4.364,12.934], alpha=0.4, cmap=cmap)
labels = np.array(["Muerto","Herido"])
cbar = plt.colorbar(ticks=range(2))
cbar.set_label('Estado', fontsize=16)
cbar.ax.set_yticklabels(['Herido', 'Muerto'],fontsize=14)
plt.ylabel("Latitude", fontsize=14)
plt.xlabel("Longitude", fontsize=14)

plt.legend(fontsize=16)
plt.tight_layout()
# plt.savefig('results/california_housing_prices_plot.pdf', format='pdf', dpi=300)
plt.title('Status of people affected by MAP in Colombia')
plt.show()

"""##### Circular diagram of the attributes of interest

The attributes of interest in this case are:
- Type of event: MAP or MUSE
- Status: Dead or Injured
- Condition
- Age Range
"""

def pastel(Xdata,atributo):

  features =Xdata[atributo].unique()

  ohe=pd.get_dummies(Xdata[atributo])

  cantidad = (ohe[features[0]]==1).sum()
  manzanas = [cantidad,(ohe[features[0]]==0).sum()]
  nombres = [features[0],features[1]]
  colores = ["#EE6055","#60D394"]
  plt.pie(manzanas, labels=nombres, autopct="%0.1f %%", colors=colores)
  print("El número de casos de " + features[0],  cantidad)

plt.figure(figsize = (7,5),dpi = 100)
plt.subplot(221)
pastel(Xdata,"Estado")
plt.subplot(222)
pastel(Xdata,"Condicion")
plt.subplot(223)
pastel(Xdata,"Rango de edad")
plt.subplot(224)
pastel(Xdata,"Tipo de evento")

"""##### Correlation between attributes

- Corr_max
- Heat map
- Hist de corr_max

- The corr function showing all correlations between attributes is applied.
"""

corr_matrix = X.corr()
corr_matrix.style.background_gradient(cmap='coolwarm')

"""- Heatmap of the attribute correlation matrix."""

import seaborn as sns
plt.figure(figsize = (10,5), dpi = 90)
sns.heatmap(corr_matrix,xticklabels=corr_matrix.columns.values,yticklabels=corr_matrix.columns.values)
plt.tight_layout()
plt.show()

"""- Histograms of the attributes."""

from pandas.plotting import scatter_matrix

attributes = [ "Tipo de evento",	"Departamento","Rango de edad",	"Condicion",	"Estado",	"Genero",
              	"Longitud_cabecera",	"Latitud_cabecera"]

scatter_matrix(X[attributes], figsize=(12, 10))
plt.tight_layout()
plt.show()

"""- From the histograms and the correlation between attributes performed previously, it is observed that there is very low correlation between attributes, only Logitude, Latitude and Department show some correlation.

 For this reason, it was decided to generate two new attributes based on the probability of occurrence of an APM given a department over time.

## **Pre-Process: Generation of two new attributes from the conditional probability**


For this it is first necessary to extract the values of the number of cases in each department for each year.

To do this, the database is searched looking for the categories of department and year in each sample to assign them to a matrix.
"""

Dpts = ["ANTIOQUIA", "META", "CAQUETA", "NORTE DE SANTANDER", "NARIÑO", "BOLIVAR", "ARAUCA", "CAUCA", "TOLIMA", "PUTUMAYO", "SANTANDER", "CORDOBA",
       "VALLE DEL CAUCA", "HUILA", "GUAVIARE", "CALDAS", "CESAR", "CUNDINAMARCA", "CHOCO", "CASANARE", "BOYACA", "SUCRE", "LA GUAJIRA",
       "MAGDALENA","VAUPES", "QUINDIO", "BOGOTA DC", "RISARALDA", "VICHADA", "ATLANTICO", "GUAINIA"]

años = [1990,2013]  #min and max year

## this code is only for checking the counter
def contadores2(Xdata,Años,dpto):
  acu = 0
  casosDpt = 0
  Años = list(range(Años[0],Años[1]+1))     ## the list of years is generated
  NAños = len(Años)                         ## number of years
  acu2 =  np.zeros((NAños,12))              ## Matrix with months per column and years per row

  for i in range(NAños):                    ## THE YEARS GO BY
    for ii in range(12):                    ## THE MONTHS GO BY

      casosDpt = ((Xdata["Departamento"] == dpto) & (Xdata["Año"] == Años[i]) & (Xdata["Mes"] == ii+1)).sum() / (Xdata["Departamento"] == dpto).sum()  ## sum of how many data meet this condition
      acu2[i][ii] = casosDpt
      acu = acu + casosDpt

  print(acu2), print(acu), print(acu2.sum())
  return acu2



## Function to organize data in rows according to a window
def sliding_window(l, w):
    if w > len(l):
        raise ValueError("Window size must be smaller or equal to the number of elements in the list.")
    t = []
    y = []
    an = len(l)-w
    i = 0
    while i< an:
      t.append(l[i:w+i])
      y.append(l[w+i])
      i += 1
    #print(an), print(len(t))
    return t, y

"""**Time series based on the probability of occurrence of an anti-personnel mine in Antioquia from 1990 to 2013**.

The conditional probability of mine occurrence given a department over a time span of 24 years is found for this case:


$$ P(X | D_M) = \frac{P(X \cap D_M)}{P(D)}  $$

$$ P(D_M) = P(D \cap A \cap M) $$

Where:

* $ $ P(D) $ represents the probability department.
* P(D_M) $ represents the probability department given a time
* A: Years
* M: Months

The above equation is merely theoretical, but for this case study the calculation of the conditional probability was done by counting.
"""

Xcal1 = contadores2(Xdata,años,"ANTIOQUIA")
Xcal1.shape[0] * Xcal1.shape[1]

mylist = Xcal1[:][:].reshape(1,-1).flatten().tolist() ## converts the entire matrix into a vector
w= 6
Xcal2, Ycal2  = sliding_window(mylist, w)  #is sent to the function as a vector and returns 2 lists
Xcal2 = np.array(Xcal2)    #the data list is converted into an array type matrix
Ycal2 = np.array(Ycal2)
#print(Xcal2[270:282][:]), print(Ycal2[270:282])
# Ycal2.shape, Xcal2.shape, Ycal2[281], Xcal2[281]
print(Xcal2),print(Ycal2)

import matplotlib.pyplot as plt

plt.figure(figsize = (16,6), dpi = 90)
plt.plot(Ycal2)
plt.xlabel('Months')
plt.ylabel('Number of cases of MAP and MUSE')
plt.title('Number of cases of MAP and MUSE by Month')
plt.grid()

maxY = Ycal2.max()
print(maxY)
mesmax =  int(np.where(Ycal2 == maxY)[0]) - int((np.where(Ycal2 == maxY)[0])/12)*12
añosmax =  1990 + int((np.where(Ycal2 == maxY)[0])/12)
print(añosmax, mesmax, "this date plus 6 months corresponds to the month with the most MAP cases.")

## check we have to add the months of the window, since Y is ahead that amount of months and X is behind, so we have to
## advance X that amount

RealFecha = ((Xdata["Departamento"] == "ANTIOQUIA") & (Xdata["Año"] == 2005) & (Xdata["Mes"] == 5)).sum() /2346
# print(Ycal2)

A = Xdata["Año"].value_counts()
print("el numero de casos desde el 2000 son: ", [A.loc[int(i+2000)] for i in range(13)])

"""# **Regression Model**

Definition of the parameters for regression.
"""

from sklearn.linear_model import ElasticNet, Lasso
from sklearn.kernel_ridge import KernelRidge
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.model_selection import ShuffleSplit, cross_val_score
from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score, cross_val_predict
from joblib import dump, load
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error as mse
from sklearn.metrics import mean_absolute_error as msa
from sklearn import svm
from sklearn.svm import SVR

steps = [
          [('scaler', StandardScaler()), ('reg',  KernelRidge(kernel = 'rbf'))], #nonlinear regressor

          [('scaler', StandardScaler()), ('reg',  SVR(kernel = 'linear'))],

          [('scaler', StandardScaler()),('reg', KNeighborsRegressor())],

          [('scaler',StandardScaler()),('reg',Lasso())],

          [('scaler',StandardScaler()),('reg',ElasticNet())]
         ]

#parameters to search by grid search
parameters =[
             {'reg__gamma':[1e-3,1e-2,1e-1,1,10],
             'reg__alpha':[1e-3,1e-2,1e-1,1,10]
             },
             {
             'reg__epsilon':[1e-3,1e-2,1e-1,1,10],
             'reg__C':[1e-3,1e-2,1e-1,1,10]
             },
             {
              'reg__n_neighbors': [ 1, 3, 5], #parameters n_neighbors must always be an int
             },

             {
              'reg__alpha': [0,1e-3,1e-2,1e-1,1,10] #For lasso
             },
             {
              'reg__alpha': [0,1e-3,1e-2,1e-1,1,10], #Elastic parameters
              'reg__l1_ratio':[0,0.25,0.5,0.75,1]
             }
              ]

label_model = ['SC_KRBF','SC_SVR_Linear','SC_KNN', 'SC_Lasso', 'SC_ElasticNet']

"""- **Training on Xtrain**"""

# Time-preserving data partitioning
Xtrain = Xcal2[:197]
Xtest = Xcal2[197:]

Ytrain = Ycal2[:197]
Ytest = Ycal2[197:]
Ytrain.shape, Ytest.shape

"""### Cross-validation"""

# nested cross-validation
Niter = 10 #number of partitions outer loop outer loop outer loop nested cross validation
msev =np.zeros((Niter,len(steps)))#arrangement to save the hit
Nmod = len(steps) #number of models to be tested
best_estimators = Niter*[None]#best method by iteration
###key to performance
best_hyperpar = Niter*[None]#best method by iteration
#############################
for j in range(Niter): #outter loop
      #print('it %d/%d'%(j+1,Niter))
      #partitioning outter loop data
      X_train, X_test, y_train, y_test = train_test_split(Xtrain,Ytrain,test_size=0.3) # xtrain 60, xtest 26
      list_est = [] #inner loop list to store best estimator per model for iteration j
      list_hyper = [] #inner loop list to store best hyperparameters per model for iteration j
      for r in range(Nmod): #I run through all possible models to be tested in iteration j of the outter loop
          grid_search = GridSearchCV(Pipeline(steps[r]), parameters[r],cv=5,verbose=5,scoring='neg_mean_absolute_error',n_jobs=5) #cv inner loop
          #generate better model
          grid_search.fit(X_train,y_train) # inner loop is performed inside the gridsearch
          #estimate outter loop test set output
          y_pred = grid_search.best_estimator_.predict(X_test)
          #Save best model
          list_est.append(grid_search.best_estimator_)
          list_hyper.append(grid_search.best_params_)
          #Save Hit
          msev[j,r] = msa(y_test,y_pred)
          print('it %d/%d-Modelo %d/%d'%(j+1,Niter,r+1,len(steps)))
          print('best hyper', grid_search.best_params_)
          print('msa:',msev[j,r])

      best_estimators[j] = list_est #Save the best models
      best_hyperpar[j] = list_hyper #Save hyperparameters


      savedata = {
          'acc':msev,
          'best_models':best_estimators,
          'best_parameters':best_hyperpar,
            }

"""###Choice of the best model:

- Since the mean absolute error is a way of quantifying the performance of each model, the box plot of the absolute error of each model is plotted and it is observed that for this database the best model is KernelRidge, since it is the one with the lowest variability with respect to the median and it is evident that it is not biased.
- Since the mean absolute error is a way of quantifying the performance of each model, the box plot of the absolute error of each model is plotted and it is observed that for this database the best model is KernelRidge, since it is the one with the lowest variability with respect to the median and it is evident that it is not biased.
"""

plt.figure(figsize = (10,5), dpi = 90)
plt.boxplot(msev)
plt.xticks(ticks=np.arange(len(steps))+1,labels=label_model)
plt.title('Box plot of the mean absolute error of each model')
plt.show()

"""###Parameter tuning"""

from scipy.stats import mode
from datetime import date

r = 0
nh = len(best_hyperpar[0][r])
hyperpar_r = np.zeros((Niter,nh))
for i in range(Niter):
  for j in range(nh):
    hyperpar_r[i,j] = best_hyperpar[i][r].get(list(best_hyperpar[i][r].keys())[j])


  #check whole number for casting
aa = list(best_hyperpar[0][r].keys())
c = -1

for i in range(len(aa)):
  if aa[i].find('n_neighbors') > -1:
    c = i

mode_hyper = mode(hyperpar_r,axis=0)[0][0]
plt.boxplot(hyperpar_r)
plt.xticks(ticks=np.arange(nh)+1,labels=list(best_hyperpar[0][r].keys()))
plt.title('Best_hyperparameters '+label_model[r])
plt.grid()
plt.show()
print(label_model[r],"bestparameter",mode_hyper)

mode_hyper

"""###Training with final model

####Xtrain and Ytrain training
"""

#Training with the best hyperparameters
model_f = Pipeline(steps[r])
paramsf = parameters[r]
hyperl = list(parameters[r].keys())
nh = len(hyperl)


for j in range(nh):
  if j == c:
    paramsf[hyperl[j]] = int(mode_hyper[j])
  else:
     paramsf[hyperl[j]] = mode_hyper[j]

model_f.set_params(**paramsf)
model_f.fit(Xtrain,Ytrain)

"""#####Prediction"""

#Graph
plt.figure(figsize = (16,6), dpi = 90)

Ypredict = model_f.predict(Xcal2)
error = Ycal2 - Ypredict
print('Mean absolute error',msa(Ycal2,Ypredict))


plt.subplot(211)
plt.plot(Ypredict,label='prediction')
plt.plot(Ycal2,'--',label='target')
plt.grid()
plt.title('Prediction with partitioning')
plt.legend()

"""####Training with all trained data"""

#Training with the best hyperparameters
model_f = Pipeline(steps[r])
paramsf = parameters[r]
hyperl = list(parameters[r].keys())
nh = len(hyperl)


for j in range(nh):
  if j == c:
    paramsf[hyperl[j]] = int(mode_hyper[j])
  else:
     paramsf[hyperl[j]] = mode_hyper[j]

model_f.set_params(**paramsf)
model_f.fit(Xcal2,Ycal2)

"""#####Prediction"""

#Graph
plt.figure(figsize = (16,6), dpi = 90)

Ypredict1 = model_f.predict(Xcal2)
error1 = Ycal2 - Ypredict1
print('Mean absolute error',msa(Ycal2,Ypredict1))

plt.subplot(211)
plt.plot(Ypredict,label='prediction')
plt.plot(Ycal2,'--',label='target')
plt.grid()
plt.title('Prediction with all trained data')
plt.legend()

plt.figure(figsize = (16,6), dpi = 90)

plt.plot(error,label='Partition error')
plt.plot(error1,label='Error with all data')
plt.grid()
plt.title('Error comparison')
plt.legend()

"""It is predicted using the model trained with Xtrain and Ytrain, i.e. with 70% of the data respecting temporality, but evaluating it on all Xcal2 and Ycal2 data. So it is evident that there is a high error in the prediction from 70% of the data (months > month #197), this is because the amount of data with which it is trained is a fraction of the total data, which in itself are already very few, therefore it is difficult for the model to make the prediction for data that it does not know, which would be the remaining 30%.

For this reason it is necessary to use all the data for training.

# Quality of the predictor

Validation of predictor quality by changing the window width for X and Y generation.
"""

def wep(w, mylist):

  Xcal2, Ycal2  = sliding_window(mylist, w)  #is sent to the function as a vector and returns 2 lists
  Xcal2 = np.array(Xcal2)    #the data list is converted into an array type matrix
  Ycal2 = np.array(Ycal2)

  #Training
  r = 0 #Kernelridge Road
  #Training with the best hyperparameters
  model_f = Pipeline(steps[r])
  paramsf = parameters[r]
  hyperl = list(parameters[r].keys())
  nh = len(hyperl)

  for j in range(nh):
    if j == c:
      paramsf[hyperl[j]] = int(mode_hyper[j])
    else:
      paramsf[hyperl[j]] = mode_hyper[j]

  model_f.set_params(**paramsf)
  model_f.fit(Xcal2,Ycal2)

  #predicción
  Ypredict = model_f.predict(Xcal2)
  error = Ycal2 - Ypredict

  #Gráfica
  plt.figure(figsize = (16,6), dpi = 90)
  plt.subplot(211)
  plt.plot(Ypredict,label='prediction')
  plt.plot(Ycal2,'--',label='target')
  plt.grid()
  plt.legend()

  plt.subplot(212)
  plt.plot(error,label='Error')
  plt.grid()
  plt.legend()
  plt.show()
  print('Mean absolute error',msa(Ycal2,Ypredict))

"""- Training and prediction with data obtained from a window w = 2

"""

wep(2, mylist)

"""- Training and prediction with data obtained from a window w = 4

"""

wep(4, mylist)

"""- Training and prediction with data obtained from a window w = 8

"""

wep(8, mylist)

"""- Training and prediction with data obtained from a window w = 12"""

wep(12, mylist)

"""### Remarks:

* It is observed how the width of the window defined by the `w` parameter has an inverse relationship with the mean absolute error, since it affects both the size of the segments and the amount of data in each one.

* It is evident that when modifying the width of the window, for values `w` < 4 the mean absolute error increases, because when the number of samples increases, that is, the number of partitions of the time series is greater, the size of the segments decreases, which implies a greater difficulty in making the prediction when there is not enough training data associated with each output "Y".

* It is important to be careful when assigning the value of the window, because although increasing this value can generate a lower mean absolute error, it could also cause negative effects on the accuracy of the predictor, implying that a larger amount of data is necessary to generate good results with the model.

#References.

[1] [Covid Notebook](https://github.com/amalvarezme/AnaliticaDatos/blob/master/2_PlantillaProyecto_EjRegresionGH/02S_S3_ValidacionCruzadaAnidada_covidCSV.ipynb).

[2] [Housing Notebook](https://github.com/amalvarezme/AnaliticaDatos/blob/master/2_PlantillaProyecto_EjRegresionGH/02S_2_Guia_LadoaLado.ipynb).

[3] [Workshop 1 FIFA](https://colab.research.google.com/drive/18ilVuDjxS7yP8rSTm5CQG7eFQhabxpcb?authuser=1).

[4] [Slides_01_02S_IntroAprendMaq](https://github.com/amalvarezme/AnaliticaDatos/blob/master/1_IntroRepasoGH/Slides_01_02S_IntroAprendMaq.pdf).

[5] []()
"""