# -*- coding: utf-8 -*-
"""Codigo_fifa21.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UcDa12upVn89p-lYcF2WUfpgY4kbqU_7

##Adjunta el archivo fifa21_male2
"""

#cargar datos desde drive acceso libre
FILEID = "1ASb2dTqlTT-S749fiyq8LQ-zKfp0Ldq1"
!wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$FILEID -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id="$FILEID -O codigos.zip && rm -rf /tmp/cookies.txt
!unzip codigos.zip
!dir

# Commented out IPython magic to ensure Python compatibility.
from scipy.stats import mode
from datetime import date


def bestomodel(r):
  nh = len(best_hyperpar[0][r])
  hyperpar_r = np.zeros((Niter,nh))
  for i in range(Niter):
    for j in range(nh):
      hyperpar_r[i,j] = best_hyperpar[i][r].get(list(best_hyperpar[i][r].keys())[j])



  #revisar numero entero para realizar casting
  aa = list(best_hyperpar[0][r].keys())
  c = -1
  for i in range(len(aa)):
      if aa[i].find('n_neighbors') > -1:
        c = i

  mode_hyper = mode(hyperpar_r,axis=0)[0][0]
  plt.boxplot(hyperpar_r)
  plt.xticks(ticks=np.arange(nh)+1,labels=list(best_hyperpar[0][r].keys()))
  plt.title('Best_hyperparameters '+label_model[r])
  plt.grid()
  plt.show()

  return print("Bestomodelo",mode_hyper)


#Paso 1: Lectura
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

#from funciones_fifa import dummy_fifa, pre_exploratorio, save_fig, code_euro

from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score, cross_val_predict
from sklearn.metrics import mean_absolute_error
from sklearn.pipeline import Pipeline
# %matplotlib inline


csv_path = 'fifa21_male2.csv'
Xdata = pd.read_csv(csv_path)
col_drop = ['ID', 'Name','ID','Player Photo','Club Logo',
            'Flag Photo','Loan Date End','Gender','Team & Contract','W/F','SM','IR','Contract']
Xdata.drop(columns = col_drop, inplace = True)
Xdata.head()

Xdata.info()

"""###Arreglo de atributos y definición de la salida"""

#Paso 2: Particion entrenamiento y validacion
# Tamaño Xtrain 70%, Tamaño Xtest 30%
import warnings
warnings.filterwarnings("ignore")

from sklearn.model_selection import train_test_split
Xtrain, Xtest = train_test_split(Xdata,test_size=0.3)
col_sal = "Release Clause"
ytrain = Xtrain[col_sal]
ytest = Xtest[col_sal]
Xtrain.drop(columns=col_sal,inplace=True)
Xtest.drop(columns=col_sal,inplace=True)

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="most_frequent")
ytrain = imputer.fit_transform(pd.DataFrame(ytrain))
ytrain = ytrain.reshape(-1)

ytest = imputer.transform(pd.DataFrame(ytest))
ytest = ytest.reshape(-1)

print(ytrain.shape, ytest.shape)

a = ytrain[0]
print(a,print(a[1:-1]))

"""###Funciones necesarias para convertir las características estadísticas y de dinero"""

#%%
#codificar estadísticas -> jugadores fifa ej 88+3
def code_stats(y):
    yc = np.zeros(y.shape[0])
    for i in range(y.shape[0]):
        #print(y.iloc[i])

        if y.iloc[i].find("+") > -1: # encontrar signo mas en str y casteo a flotante
            yc[i] = float(y.iloc[i][:y.iloc[i].find("+")])+float(y.iloc[i][y.iloc[i].find("+")+1:])
        else: yc[i] = float(y.iloc[i])
        #print(yc[i])
    return yc

#codificar moneda ->
def code_euro(y):
    yc = np.zeros(y.shape[0])
    for i in range(y.shape[0]):
      #  print(i,y[i])
        if y[i][-1]=='M': yc[i] = float(y[i][1:-1])*10**6 #buscar M y reemplazar 10^6
        elif y[i][-1]=='K': yc[i] = float(y[i][1:-1])*10**3 # buscar K y reemplazar por 10^3
        else: yc[i] = float(y[i][1:])
        #print(yc[i])
    return yc

#codificar salida
ytrain_num = code_euro(ytrain)
ytest_num = code_euro(ytest)

ytrain_num[:5]

"""En la siguiente celda se obtiene un histograma y un diagrama de caja que muestran los valores más frecuentes de la salida "ytrain_num"
"""

import matplotlib.pyplot as plt
plt.hist(ytrain_num,bins = 100)
plt.show()

plt.boxplot(ytrain_num)
plt.show()

"""Observando así que hay valores muy atípicos, sin embrago, no son tantos, lo cual es bueno porque no afectará nuestro análisis.

A continuación se establecen listas de categorías como las que son dinero, estadísticas y categóricas para poder realizar el preproceso debido a cada una de ellas.


"""

#definir columnas tipo string  para codificar moneda, estadistica fifa y categoricas
col_euro = ['Value','Wage'];
col_stats = ['LS','ST','RS','LW','LF','CF','RF','RW','LAM',
             'CAM','RAM','LM','LCM','CM','RCM','RM','LWB',
             'LDM','CDM','RDM','RWB','LB','LCB','CB','RCB','RB','GK'];

cat = ['Nationality','Club','BP','Position','foot','A/W','D/W']
items = []
for i in cat:
    items += [list(Xdata[i].value_counts().index)]# ['Alto','Medio','Bajo']
cat_usr = dict(zip(cat, items))

"""# Crear clase propia de preproceso para Fifa

En la siguiente celda, se crea una clase que va a organizar todos nuestros atributos dejándolos en valores numéricos dependiendo de su categoría definida anteriormente, esto con el fin de que la interpretabilidad sea mucho más fácil y los algorítmos con el computador salgan más sencillos.
"""

#%% dummy transformer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder,OneHotEncoder

class dummy_fifa(BaseEstimator,TransformerMixin):
    #inicializacion de clase y varaibles
    def __init__(self, col_euro,col_stats, cat_usr): #constructor clase
        self.col_euro = col_euro #lista atributos tipo moneda
        self.col_stats = col_stats #lista atributos tipo estadistica
        self.cat_usr = cat_usr #lista de atributos categoricos


    def fit(self,X, *_):
        Xi = X.copy() #copiar dataset para no reemplazar original
        self.imputer_num = SimpleImputer(strategy="most_frequent") #crear imputador tipo modo
        self.a = Xi.columns[np.sum(Xi.isna())> 0] #encontrar columnas con datos faltantes
        #print(a)
        self.imputer_num.fit(Xi[self.a]) # ajustar imputador

        Xi[self.a] = self.imputer_num.transform(Xi[self.a]) #evaluar  sobre datos imputador
        #print('Cod Euros\n')
        for i in self.col_euro: #codificar tipo moneda
         #   print(i)
            Xi[i] = code_euro(np.array(Xi[i]))
        #print('Cod stats\n')
        for i in self.col_stats: #codificar datos estadisticos
         #   print(i)
            Xi[i] = code_stats(Xi[i])

        #height, wieght corregir formato
        Xi['Height'].replace(regex=["'"], value='.',inplace=True)
        Xi['Height'].replace(regex=['"'], value='0',inplace=True)
        for i in Xi.index:
            #print(float(Xi.loc[i,'Weight'][:-3]))
            Xi.loc[i,'Weight'] = float(Xi.loc[i,'Weight'][:-3])
            Xi.loc[i,'Height'] = float(Xi.loc[i,'Height'])

        Xi['Height'] = Xi['Height'].astype('float64');#print(Xi['Height'].dtype)
        Xi['Joined'] = Xi['Joined'].replace(regex=", ",value="")

        Xi['Joined'] = Xi['Joined'].replace(regex="Jan ",value="1")
        Xi['Joined'] = Xi['Joined'].replace(regex="Feb ",value="2")
        Xi['Joined'] = Xi['Joined'].replace(regex="May ",value="5")

        Xi['Joined'] = Xi['Joined'].replace(regex="Mar ",value="3")
        Xi['Joined'] = Xi['Joined'].replace(regex="Jun ",value="6")
        Xi['Joined'] = Xi['Joined'].replace(regex="Jul ",value="7")

        Xi['Joined'] = Xi['Joined'].replace(regex="Apr ",value="4")
        Xi['Joined'] = Xi['Joined'].replace(regex="Aug ",value="8")
        Xi['Joined'] = Xi['Joined'].replace(regex="Sep ",value="9")

        Xi['Joined'] = Xi['Joined'].replace(regex="Oct ",value="10")
        Xi['Joined'] = Xi['Joined'].replace(regex="Nov ",value="11")
        Xi['Joined'] = Xi['Joined'].replace(regex="Dec ",value="12")

        Xi['Joined'] = Xi['Joined'].astype('float64')

        Xi['Hits'] = Xi['Hits'].replace(regex="K",value="")
        Xi['Hits'] = Xi['Hits'].astype('float64')
        Xi['Hits'] = Xi['Hits']*(Xi['Hits']!= Xi['Hits'].astype('int64') )*10**3 + Xi['Hits']*(Xi['Hits']==Xi['Hits'].astype('int64'))



        cat = [] #codificar variables categoricas con ordinal encoder
        for i in self.cat_usr.keys():
            cat = cat + [[*self.cat_usr.get(i)]]
        self.col_cat_usr = OrdinalEncoder(categories=cat)
        Xi[[*self.cat_usr.keys()]] =self.col_cat_usr.fit_transform(Xi[[*self.cat_usr.keys()]])

        #self.col_cat = Xi.columns[Xi.dtypes=='O']
        #self.cod = OrdinalEncoder()
        #self.cod.fit(Xi[self.col_cat])

        return self

    def transform(self, X, *_):#funcion transformador
        Xi = X.copy()
        Xi[self.a] = self.imputer_num.transform(Xi[self.a])

        for i in self.col_euro:
         #   print(i)
            Xi[i] = code_euro(np.array(Xi[i]))
        #print('Cod stats\n')
        for i in self.col_stats:
         #   print(i)
            Xi[i] = code_stats(Xi[i])

        #height, wieght
        Xi['Height'].replace(regex=["'"], value='.',inplace=True)
        Xi['Height'].replace(regex=['"'], value='0',inplace=True)
        for i in Xi.index:
            #print(float(Xi.loc[i,'Weight'][:-3]))
            Xi.loc[i,'Weight'] = float(Xi.loc[i,'Weight'][:-3])
            Xi.loc[i,'Height'] = float(Xi.loc[i,'Height'])

        Xi['Height'] = Xi['Height'].astype('float64')
        Xi['Weight'] = Xi['Weight'].astype('float64')

        Xi['Joined'] = Xi['Joined'].replace(regex=", ",value="")

        Xi['Joined'] = Xi['Joined'].replace(regex="Jan ",value="1")
        Xi['Joined'] = Xi['Joined'].replace(regex="Feb ",value="2")
        Xi['Joined'] = Xi['Joined'].replace(regex="May ",value="5")

        Xi['Joined'] = Xi['Joined'].replace(regex="Mar ",value="3")
        Xi['Joined'] = Xi['Joined'].replace(regex="Jun ",value="6")
        Xi['Joined'] = Xi['Joined'].replace(regex="Jul ",value="7")

        Xi['Joined'] = Xi['Joined'].replace(regex="Apr ",value="4")
        Xi['Joined'] = Xi['Joined'].replace(regex="Aug ",value="8")
        Xi['Joined'] = Xi['Joined'].replace(regex="Sep ",value="9")

        Xi['Joined'] = Xi['Joined'].replace(regex="Oct ",value="10")
        Xi['Joined'] = Xi['Joined'].replace(regex="Nov ",value="11")
        Xi['Joined'] = Xi['Joined'].replace(regex="Dec ",value="12")

        Xi['Joined'] = Xi['Joined'].astype('float64')

        Xi['Hits'] = Xi['Hits'].replace(regex="K",value="")
        Xi['Hits'] = Xi['Hits'].astype('float64')
        Xi['Hits'] = Xi['Hits']*(Xi['Hits']!= Xi['Hits'].astype('int64') )*10**3 + Xi['Hits']*(Xi['Hits']==Xi['Hits'].astype('int64'))

        Xi[[*self.cat_usr.keys()]] =self.col_cat_usr.transform(Xi[[*self.cat_usr.keys()]])
        #Xi[self.col_cat]= self.cod.transform(Xi[self.col_cat])
        return Xi

    def fit_transform(self,X,*_):
        self.fit(X)
        return self.transform(X)

dummy = dummy_fifa(col_euro=col_euro,col_stats=col_stats,cat_usr = cat_usr)
Xtrain_pre = dummy.fit_transform(Xtrain)

Xtrain_pre.info()

"""# Predicción variable de interés desde datos preprocesados"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
sca = StandardScaler()
Xtrain_pre_z = sca.fit_transform(Xtrain_pre)

Xtrain_pre_z.var(axis=0)

Xtrain_pre.var(axis=0)

red = PCA()
zz = red.fit_transform(Xtrain_pre_z)

"""#Análisis Descriptivo

En las siguientes celdas se hará un análisis por medio de gráficas como la dada por PCA, la matriz de correlaciones y algunos histogramas, para poder realizar algunas observaciones que pueden conllevar a generar un código mucho más eficaz para el desarrollo del análisis de datos.
"""

plt.scatter(zz[:,0],zz[:,1],c=ytrain_num,s = 100*Xtrain_pre['Value']/(Xtrain_pre['Value'].max()))
plt.colorbar()
plt.show()

# proceso con test
Xtest_pre = dummy.transform(Xtest)

Xtest_pre['Value']

Xtest_pre.info()

#Xtest_pre_z = sca.transform(Xtest_pre) # standardizacion
#zztest = red.transform(Xtest_pre_z) # llevarlo a 2D con pca
zztest = red.transform(sca.transform(dummy.transform(Xtest))) # una sola linea

plt.scatter(zz[:,0],zz[:,1],c=ytrain_num,s = 100*Xtrain_pre['Value']/(Xtrain_pre['Value'].max()),label='train')
plt.colorbar()
plt.scatter(zztest[:,0],zztest[:,1],c=ytest_num,s=100*Xtest_pre['Value']/Xtest_pre['Value'].max(),marker='d',label='test')
plt.legend()
plt.show()

"""###Análisis de correlación"""

corr_matrix = Xtrain_pre.corr()    ## se aplica la función corr que muestra todas las correlaciones entre los atributos
corr_matrix.style.background_gradient(cmap='coolwarm')

import seaborn as sns
plt.figure(figsize=(10, 5), dpi=90)
sns.heatmap(corr_matrix,xticklabels=corr_matrix.columns.values,yticklabels=corr_matrix.columns.values)
plt.tight_layout()
# plt.savefig('results/corr_matrix.pdf', format='pdf', dpi=300)
plt.show()

"""Con esto se observa la alta correlación que tienen algunos atributos como lo es BOV con OVA, entonces esto significa que estos datos nos están dando una información similar, por lo que para optimizar se podría generar una nueva columna de esos datos y eliminar los ya tenidos. Sin embargo, dejamos los los atributos para tener mayor número de características.

A continuación tenemos la gráfica de correlacciones de OVA con BOV mostrado su amplia correlación.
"""

plt.scatter(Xtrain_pre['OVA'],Xtrain_pre['BOV'])

"""A continuación se muestra la correlación que tienen algunas carácterísticas con otras, de las cuales resalta la linealidad que se observa en el anterior caso y en BOV con Base Stats, por lo que con el siguiente código podemos ver la relación que tienen algunos atributos con otros distintos y observar si se puede optimizar la base de datos quitando columnas que no me generen información adicional."""

from pandas.plotting import scatter_matrix

attributes = ["Skill", "foot", "Hits",
              "Age","Value","Height", "OVA", "Base Stats", "BOV", "Positioning"]
scatter_matrix(Xtrain_pre[attributes], figsize=(12, 8))
plt.tight_layout()
# plt.savefig('results/scatter_matrix_plot.pdf', format='pdf', dpi=300)
plt.show()

"""#Escoger mejor modelo

En las siguientes celdas se escogerá el mejor modelo entre (Regresión lineal, Lasso, Rígido, ElasticNet y KernelRidge) utilizando sintonización de parámetros por búsqueda por grilla, de manera que nos arroje un determinado error para cada modelo, luego hallamos el promedio de errores y se obtiene el mejor modelo basándose en el menor valor de error final. Y el hyperparámetro se lo obtiene con la moda del mejor usado en cada iteración.
"""

# definir modelos de predicción
from sklearn.linear_model import LinearRegression, ElasticNet, Ridge, Lasso
from sklearn.kernel_ridge import KernelRidge

steps = [
         [('scaler', StandardScaler()), #estandarizar cada atributo columna de xtrain centrada en 0 y var = 1
          ('reg', LinearRegression())],

         [('scaler',StandardScaler()),
          ('reg',Ridge())],

         [('scaler',StandardScaler()),
          ('reg',Lasso())],

         [('scaler',StandardScaler()),
          ('reg',ElasticNet())],

         [('scaler', StandardScaler()), #estandarizar cada atriuto columna de xtrain centrada en 0 y var = 1
          ('reg',  KernelRidge(kernel = 'rbf'))] #clasificador
         ]

#parametros a buscar por busqueda por grilla
parameters = [
             {'reg__fit_intercept':[True, False]   # para el linearregression
             },
             {
              'reg__alpha': [0,1e-3,1e-2,1e-1,1,10] #Para el ridge
             },
             {
              'reg__alpha': [0,1e-3,1e-2,1e-1,1,10] #Para el Lasso
             },
             {
              'reg__alpha': [0,1e-3,1e-2,1e-1,1,10], #parametros n_neighbors debe ser siempre un int
              'reg__l1_ratio':[0,0.25,0.5,0.75,1]
             },
             {
              'reg__alpha': [0,1e-3,1e-2,1e-1,1,10] #Para el Kernel
             }
           ]

label_model = ['Nor+RegLin','Nor+Ridge','Nor+Lasso','Nor+ElasticNet','Nor+KernelRidge']

parameters

import os
pathpre = 'datospre'

try:
  os.mkdir(pathpre)
except:
  print("Carpeta results ya existe")

#valildacion cruzada anidada
from joblib import dump, load
from sklearn.metrics import mean_absolute_error as msa
Niter = 2 #numero particiones outter loop nested cross-validation
msev =np.zeros((Niter,len(steps)))#arreglo para guardar acierto/error
Nmod = len(steps) #numero de modelos a probar
best_estimators = Niter*[None]#mejor metodo por iteracion
###clave del funcionamiento
best_hyperpar = Niter*[None]#mejor metodo por iteracion


#############################
for j in range(Niter): #outter loop # SI TIENE MENOS DE 1000 DATOS BORRAR ESTE CICLO SOLO CV EN LINEA 16
      #print('it %d/%d'%(j+1,Niter))
      #particiono datos outter loop
      X_trainj, X_testj, y_trainj, y_testj = train_test_split(Xtrain_pre,ytrain_num,test_size=0.3) # xtrain 60, xtest 26
      list_est = [] #lista lazo interno para guardar mejor estimador por modelo para iteracion j
      list_hyper = [] #lista lazo interno para guardar mejores hyperparametros por modelo para iteracion j
      for r in range(Nmod): #recorro todos los posibles modelos a probar en iteracion j del outter loop
          grid_search = GridSearchCV(Pipeline(steps[r],memory=pathpre), parameters[r],cv=5,verbose=5,scoring='neg_mean_absolute_error',n_jobs=-1) #cv inner loop
          #xtrain gridsearchcv xtrain split en 12 / cv, 60/5 = 12, xtrain 48 datos validar 12
          # cv = N -> leave one out N <30
          #generar mejor modelo
          grid_search.fit(X_trainj,y_trainj)
          #estimar salida conjunto de test
          y_pred = grid_search.best_estimator_.predict(X_testj)
          #guardar mejor modelo
          list_est.append(grid_search.best_estimator_)
          list_hyper.append(grid_search.best_params_)
          #guardar acierto
          msev[j,r] = msa(y_testj,y_pred)
          print('it %d/%d-Modelo %d/%d'%(j+1,Niter,r+1,len(steps)))
          print('best hyper', grid_search.best_params_)
          print('msa:',msev[j,r])

      best_estimators[j] = list_est #guardar mejores modelos
      best_hyperpar[j] = list_hyper #mejores hyperparametros


      savedata = {
          'acc':msev,
          'best_models':best_estimators,
          'best_parameters':best_hyperpar,
            }
      dump(savedata,'fifa.joblib')

"""##Sintonización de Parámetros

Con los siguientes diagramas de caja se puede observar cuáles valores de parámetros para cada modelo son los mejores y se obtiene una moda para mostrar el mejor valor de cada modelo denotado por "Bestomodelo [  $\alpha$  ]" donde "$\alpha$" es el valor del hyperparámetro.
"""

bestomodel(0)

bestomodel(1)

bestomodel(2)

bestomodel(3)

bestomodel(4)

"""Ahora bien, se exponen los errores obtenidos en cada iteración para cada modelo"""

msev #Errores por modelo

plt.figure(figsize=(10, 5), dpi=90)
plt.boxplot(msev)
plt.xticks(ticks=np.arange(len(steps))+1,labels=label_model)
plt.show()

"""De lo anterior se recopila el error de cada modelo y se halla su promedio, para compararlos y elegir el que provea un menor error, de manera que al final se trabaje con el modelo que arroje el menor valor de error."""

error_metodo = np.zeros(5)
for m in range(5):
  error1 = np.zeros(Niter)
  for n in range(Niter):
    error1[n] = msev[n][m]
    # print(error1)
  error_metodo[m] = np.mean(error1)
  # print(error_metodo)
error_metodo

AA=0
min = error_metodo[0]
for c in range(len(error_metodo)-1):
  if error_metodo[c]>error_metodo[c+1]:
    min = error_metodo[c+1]
    ult = c+1

print("El mejor modelo dado por el criterio de error mínimo es:", label_model[ult], "\nCon un error de:", min)

"""# Modelo definitivo

Con los mejores hiperparámetros entreno sobre Xtrain con el modelo final que se obtiene anteriormente.
"""

r = 1 #camino elastic net
steps_final = [('scaler',StandardScaler()),
               ('reg',ElasticNet(alpha=0.01,l1_ratio=0))]

modelo_final = Pipeline(steps_final)
modelo_final.fit(Xtrain_pre,ytrain_num)

"""# Caso real evaluación datos nuevos

Por último, probamos nuestro modelo final para predecir los valores a la salida con unos nuevos datos.
"""

ytest_e = modelo_final.predict(dummy.transform(Xtest)) #simular casos nuevos

print('MAE_test=', msa(ytest_num,ytest_e))

# Xtrain_pre.plot(kind="scatter", x="Age", y="Release Clause",alpha=0.1)
vmedian = np.linspace(-20,20,len(ytest_e)).reshape(-1,1)
plt.plot(vmedian, ytest_e, 'r',linewidth=4)
# plt.axis([-20, 20, 0, 5])
plt.show()

"""A continuación se muestra la gráfica donde se observa la presición que tendrá nuestro modelo para una entrada nueva. De la cual se observa que el modelo no es 100% fiel, pero su error es muy bajo."""

plt.plot(ytest_e,label='prediction')
plt.plot(ytest_num,label='target', alpha=0.5)
plt.grid()
plt.legend()
plt.show()

"""##Fuentes

1. https://github.com/amalvarezme/AnaliticaDatos
2. https://scikit-learn.org/stable/modules/linear_model.html
"""